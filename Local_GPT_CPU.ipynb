{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrx-Sachin/Local-LLM_Chatbot/blob/main/Local_GPT_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2USsN1GK7RLV"
      },
      "source": [
        "# Secure LLM ChatBot ðŸ““ ðŸ¤–\n",
        "\n",
        "A free-to-use, locally running, privacy-aware chatbot. No GPU or internet required. ChatBot build on streamlit and GPT4All\n",
        "<br/>\n",
        "\n",
        "Made with â¤ï¸ by Sachin\n",
        "\n",
        "[GitHub](https://github.com/Mrx-Sachin/Local-LLM_Chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "o_tMLgj0XT8A"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ‘‡ Installing dependencies\n",
        "%%capture\n",
        "# Install langchain\n",
        "!pip install langchain\n",
        "\n",
        "# Install vectorStore\n",
        "!pip install faiss-cpu\n",
        "\n",
        "# Install gpt4all\n",
        "!pip install gpt4all\n",
        "\n",
        "# Install huggingfaceHub\n",
        "!pip install huggingface-hub\n",
        "\n",
        "# Install PyPdf for working with PDFs\n",
        "!pip install pypdf\n",
        "!pip install streamlit\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyzT9NeHXjVO"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ‘‡ Selecting Model { form-width: \"20%\", display-mode: \"form\" }\n",
        "#@markdown ---\n",
        "#@markdown - **Select Model** - A list of LLM models to choose from.\n",
        "#@markdown\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from gpt4all import GPT4All\n",
        "# Instantiate GPT4All with the desired model\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "modelname=widgets.Dropdown(\n",
        "    options=[\"wizardlm-13b-v1.2.Q4_0.gguf\",\n",
        "             \"mistral-7b-openorca.Q4_0.gguf\",\n",
        "             \"gpt4all-falcon-newbpe-q4_0.gguf\",\n",
        "             \"orca-mini-3b-gguf2-q4_0.gguf\",\n",
        "             \"gpt4all-13b-snoozy-q4_0.gguf\",\n",
        "             \"replit-code-v1_5-3b-newbpe-q4_0.gguf\"\n",
        "             ],\n",
        "    value='orca-mini-3b-gguf2-q4_0.gguf',\n",
        "      description=\"Select model:\"\n",
        "    )\n",
        "\n",
        "# model=GPT4All(modelname)\n",
        "##below code it to auto downlode the selected model\n",
        "# def load_model(selected_model_name):\n",
        "#     if selected_model_name['type'] == 'change' and selected_model_name['name'] == 'value':\n",
        "#         try:\n",
        "#             # print(selected_model_name['new'])\n",
        "#             model = GPT4All(selected_model_name)\n",
        "#             # Proceed with model interaction\n",
        "#         except Exception as e:\n",
        "#             print(f\"Model loading failed: {e}\")\n",
        "# # print(modelname.value)\n",
        "# modelname.observe(load_model)\n",
        "\n",
        "display(modelname)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xk37uz4vfxV"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ‘‡ Downloading Model { form-width: \"20%\", display-mode: \"form\" }\n",
        "print(modelname.value)\n",
        "model=GPT4All(model_name=modelname.value)\n",
        "# Open a file in write mode\n",
        "with open(\"model.txt\", \"w\") as f:\n",
        "    # Write the name to the file\n",
        "    f.write(modelname.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuTf5tosXjQJ"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ‘‡Load the  Model Requirement { form-width: \"20%\", display-mode: \"form\" }\n",
        "%%writefile chat.py\n",
        "import streamlit as st\n",
        "import random\n",
        "import time\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "st.title(\"Simple chat\")\n",
        "st.sidebar.title(\"Chat History\")\n",
        "with open(\"model.txt\", \"r\") as f:\n",
        "    modelname = f.read()\n",
        "model=GPT4All(model_name=modelname)\n",
        "# model = GPT4All('wizardlm-13b-v1.2.Q4_0.gguf')\n",
        "# model = GPT4All('orca-mini-3b-gguf2-q4_0.gguf')\n",
        "with model.chat_session():\n",
        "  # Initialize chat history\n",
        "  if \"messages\" not in st.session_state:\n",
        "      st.session_state.messages = []\n",
        "\n",
        "  # Display chat messages from history on app rerun\n",
        "  for message in st.session_state.messages:\n",
        "      with st.chat_message(message[\"role\"]):\n",
        "          st.markdown(message[\"content\"])\n",
        "\n",
        "  # Accept user input\n",
        "\n",
        "\n",
        "\n",
        "  if prompt := st.chat_input(\"What is up?\"):\n",
        "  # if prompt:\n",
        "      # Add user message to chat history\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        # Display user message in chat message container\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "        # Display assistant response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            message_placeholder = st.empty()\n",
        "            full_response = \"\"\n",
        "            # assistant_response=\"\"\n",
        "            # assistant_response=prompt\n",
        "\n",
        "            assistant_response = model.generate(prompt=prompt, temp=0)\n",
        "            # assistant_response = model.generate(prompt=prompt, temp=0)\n",
        "            # Simulate stream of response with milliseconds delay\n",
        "            for chunk in assistant_response.split():\n",
        "                full_response += chunk + \" \"\n",
        "                time.sleep(0.05)\n",
        "                # Add a blinking cursor to simulate typing\n",
        "                message_placeholder.markdown(full_response + \"â–Œ\")\n",
        "            message_placeholder.markdown(full_response)\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ih6-L42dXjKv"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ‘‡ Start your personal Secure Bot { form-width: \"20%\", display-mode: \"form\" }\n",
        "#@markdown ---\n",
        "#@markdown -  Copy the Ip address .\n",
        "#@markdown -  Open the link  .\n",
        "#@markdown -  Paste your given Ip and pastes it inside Tunnel Password: .\n",
        "#@markdown\n",
        "#@markdown ---\n",
        "# !python -m streamlit run chat.py\n",
        "!streamlit run chat.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGy724Yo5N5M4XIrbSraNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}