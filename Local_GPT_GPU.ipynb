{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2USsN1GK7RLV"
      },
      "source": [
        "# Secure LLM ChatBot üìì ü§ñ\n",
        "\n",
        "A free-to-use, locally running, privacy-aware chatbot. Internet required. ChatBot build on streamlit and GPT4All\n",
        "<br/>\n",
        "Using GPU for faster response\n",
        "<br/>\n",
        "\n",
        "Made with ‚ù§Ô∏è by Sachin\n",
        "\n",
        "[GitHub](https://github.com/Mrx-Sachin/Local-LLM_Chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "o_tMLgj0XT8A"
      },
      "outputs": [],
      "source": [
        "#@title üëá Installing dependencies\n",
        "%%capture\n",
        "# Install langchain\n",
        "!pip install langchain\n",
        "\n",
        "# Install vectorStore\n",
        "!pip install faiss-cpu\n",
        "\n",
        "# Install gpt4all\n",
        "!pip install gpt4all\n",
        "\n",
        "# Install huggingfaceHub\n",
        "!pip install huggingface-hub\n",
        "\n",
        "# Install PyPdf for working with PDFs\n",
        "!pip install pypdf\n",
        "!pip install streamlit\n",
        "!npm install localtunnel\n",
        "!apt install libvulkan1\n",
        "\n",
        "# install nvidia vulkan driver\n",
        "!apt install libnvidia-gl-525-server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2c69a7128c7e4d1c81e0bafe759c9643",
            "49407c60ae3a4b909b172eb409b879e8",
            "0d2c9746c31e44afb685d958cc9f349a"
          ]
        },
        "id": "jyzT9NeHXjVO",
        "outputId": "cd95fde8-cb9f-463b-c6e3-14817a78a96b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c69a7128c7e4d1c81e0bafe759c9643",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dropdown(description='Select model:', index=3, options=('wizardlm-13b-v1.2.Q4_0.gguf', 'mistral-7b-openorca.Q4‚Ä¶"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title üëá Selecting Model { form-width: \"20%\", display-mode: \"form\" }\n",
        "#@markdown ---\n",
        "#@markdown - **Select Model** - A list of LLM models to choose from.\n",
        "#@markdown\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from gpt4all import GPT4All\n",
        "# Instantiate GPT4All with the desired model\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "modelname=widgets.Dropdown(\n",
        "    options=[\"wizardlm-13b-v1.2.Q4_0.gguf\",\n",
        "             \"mistral-7b-openorca.Q4_0.gguf\",\n",
        "             \"gpt4all-falcon-newbpe-q4_0.gguf\",\n",
        "             \"orca-mini-3b-gguf2-q4_0.gguf\",\n",
        "             \"gpt4all-13b-snoozy-q4_0.gguf\",\n",
        "             \"replit-code-v1_5-3b-newbpe-q4_0.gguf\",\n",
        "             \"nous-hermes-llama2-13b.Q4_0.gguf\"\n",
        "             ],\n",
        "    value='orca-mini-3b-gguf2-q4_0.gguf',\n",
        "      description=\"Select model:\"\n",
        "    )\n",
        "\n",
        "# model=GPT4All(modelname)\n",
        "##below code it to auto downlode the selected model\n",
        "# def load_model(selected_model_name):\n",
        "#     if selected_model_name['type'] == 'change' and selected_model_name['name'] == 'value':\n",
        "#         try:\n",
        "#             # print(selected_model_name['new'])\n",
        "#             model = GPT4All(selected_model_name)\n",
        "#             # Proceed with model interaction\n",
        "#         except Exception as e:\n",
        "#             print(f\"Model loading failed: {e}\")\n",
        "# # print(modelname.value)\n",
        "# modelname.observe(load_model)\n",
        "\n",
        "display(modelname)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xk37uz4vfxV",
        "outputId": "1f1ba079-47b3-4d2e-b5be-0caa8d6987cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wizardlm-13b-v1.2.Q4_0.gguf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.37G/7.37G [02:00<00:00, 61.2MiB/s]\n",
            "Verifying: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.37G/7.37G [00:32<00:00, 224MiB/s]\n"
          ]
        }
      ],
      "source": [
        "#@title üëá Downloading Model { form-width: \"20%\", display-mode: \"form\" }\n",
        "print(modelname.value)\n",
        "model=GPT4All(model_name=modelname.value)\n",
        "# Open a file in write mode\n",
        "with open(\"model.txt\", \"w\") as f:\n",
        "    # Write the name to the file\n",
        "    f.write(modelname.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuTf5tosXjQJ",
        "outputId": "a358cec5-e639-459a-c203-0241f79a25d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing chat.py\n"
          ]
        }
      ],
      "source": [
        "#@title üëáLoad the  Model Requirement { form-width: \"20%\", display-mode: \"form\" }\n",
        "%%writefile chat.py\n",
        "import streamlit as st\n",
        "import random\n",
        "import time\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "st.title(\"Secure chat\")\n",
        "st.sidebar.title(\"Chat History\")\n",
        "with open(\"model.txt\", \"r\") as f:\n",
        "    modelname = f.read()\n",
        "model=GPT4All(model_name=modelname,device='gpu')\n",
        "# model = GPT4All('wizardlm-13b-v1.2.Q4_0.gguf')\n",
        "# model = GPT4All('orca-mini-3b-gguf2-q4_0.gguf')\n",
        "with model.chat_session():\n",
        "  # Initialize chat history\n",
        "  if \"messages\" not in st.session_state:\n",
        "      st.session_state.messages = []\n",
        "\n",
        "  # Display chat messages from history on app rerun\n",
        "  for message in st.session_state.messages:\n",
        "      with st.chat_message(message[\"role\"]):\n",
        "          st.markdown(message[\"content\"])\n",
        "\n",
        "  # Accept user input\n",
        "  if prompt := st.chat_input(\"What is up?\"):\n",
        "  # if prompt:\n",
        "      # Add user message to chat history\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        # Display user message in chat message container\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(prompt)\n",
        "        # Display assistant response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            message_placeholder = st.empty()\n",
        "            full_response = \"\"\n",
        "            # assistant_response=\"\"\n",
        "            # assistant_response=prompt\n",
        "\n",
        "            assistant_response = model.generate(prompt=prompt, temp=0)\n",
        "            # assistant_response = model.generate(prompt=prompt, temp=0)\n",
        "            # Simulate stream of response with milliseconds delay\n",
        "            for chunk in assistant_response.split():\n",
        "                full_response += chunk + \" \"\n",
        "                time.sleep(0.05)\n",
        "                # Add a blinking cursor to simulate typing\n",
        "                message_placeholder.markdown(full_response + \"‚ñå\")\n",
        "            message_placeholder.markdown(full_response)\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0KiuvRi-i3a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gpt4all\n",
        "!pip install langchain\n",
        "!apt install libvulkan1\n",
        "\n",
        "# install nvidia vulkan driver\n",
        "!apt install libnvidia-gl-525-server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wZnHe158-Ja",
        "outputId": "3ec772e4-be18-4f73-895f-9bb012a53207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU available.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    # Get the properties of the first GPU\n",
        "    gpu_properties = torch.cuda.get_device_properties(0)\n",
        "\n",
        "    # Print GPU name and memory\n",
        "    print(f\"GPU Name: {gpu_properties.name}\")\n",
        "    print(f\"GPU Memory: {gpu_properties.total_memory / 1024**2} MB\")\n",
        "else:\n",
        "    print(\"No GPU available.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8lEYwkZ9GOy"
      },
      "outputs": [],
      "source": [
        "from gpt4all import GPT4All\n",
        "# model = GPT4All('wizardlm-13b-v1.2.Q4_0.gguf',device='Tesla T4')\n",
        "# from gpt4all import GPT4All\n",
        "model = GPT4All('wizardlm-13b-v1.2.Q4_0.gguf',device='gpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "yi4Wmp6p9PUc",
        "outputId": "64d49951-7c22-45fa-ea39-a8ffeb17fb8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You can start chatting with the model. Type 'exit' to end the conversation.\n",
            "You: hii my name is raveesh \n",
            "Model: Hello Raveesh! It's nice to meet you. How can I assist you today?\n",
            "You: can yo tell me my name \n",
            "Model: Of course, your name is Raveesh.\n",
            "You: can you guess my sername\n",
            "Model: I'm sorry for the confusion earlier, but without additional context or information, I cannot accurately guess your surname. Could you please provide more details or context?\n",
            "You: i already told you my name \n",
            "Model: Apologies for misunderstanding your question earlier. As per our previous conversation, your full name is Raveesh. If you're asking about your surname, it would be helpful if you could confirm that by providing more context or information.\n",
            "You: i was asking to to predict my surname but just let it go its yadav .\n",
            "Model: Thank you for the clarification, Raveesh Yadav! If you have any other questions or need assistance, feel free to ask.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a5ee69a49de2>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Start the chat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mchat_with_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-a5ee69a49de2>\u001b[0m in \u001b[0;36mchat_with_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m# Check for exit condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# @title cli chat\n",
        "def chat_with_model():\n",
        "    print(\"You can start chatting with the model. Type 'exit' to end the conversation.\")\n",
        "\n",
        "    with model.chat_session():\n",
        "        while True:\n",
        "            user_input = input(\"You: \")\n",
        "\n",
        "            # Check for exit condition\n",
        "            if user_input.lower() == 'exit':\n",
        "                print(\"Conversation ended.\")\n",
        "                break\n",
        "\n",
        "            # Generate model response based on user input\n",
        "            model_response = model.generate(prompt=user_input, temp=0)\n",
        "\n",
        "            # Display model's response\n",
        "            print(\"Model:\", model_response)\n",
        "\n",
        "# Start the chat\n",
        "chat_with_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih6-L42dXjKv",
        "outputId": "b2cf186c-95ba-46d5-f55b-0d7d173ef6d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.80.73.178\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.267s\n",
            "your url is: https://tender-teams-switch.loca.lt\n"
          ]
        }
      ],
      "source": [
        "#@title üëá Start your personal Secure Bot { form-width: \"20%\", display-mode: \"form\" }\n",
        "#@markdown ---\n",
        "#@markdown -  Copy the Ip address .\n",
        "#@markdown -  Open the link  .\n",
        "#@markdown -  Paste your given Ip and pastes it inside Tunnel Password: .\n",
        "#@markdown\n",
        "#@markdown ---\n",
        "# !python -m streamlit run chat.py\n",
        "!streamlit run chat.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXxlM4-Wrvuk",
        "outputId": "127bc548-4e0b-4f41-b6b3-d34c00b56be7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.98G/1.98G [00:30<00:00, 65.9MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': '### System:\\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': ' Hello! How may I assist you today?'}, {'role': 'user', 'content': 'write me a short poem'}, {'role': 'assistant', 'content': \" Sure, here's a short poem for you: \\n\\nBeneath the blue sky so bright, \\nA world awaits with endless scope. \\nThe sun shines down on every hand, \\nAnd love and laughter fill the air. \\n\\nOh, how I long to be there, \\nTo experience all that life can handle. \\nBut for now, here's my poem, \\nA token of my deepest feeling.\"}, {'role': 'user', 'content': 'thank you'}, {'role': 'assistant', 'content': \" You're welcome! Is there anything else I can help you with?\"}]\n"
          ]
        }
      ],
      "source": [
        "from gpt4all import GPT4All\n",
        "model = GPT4All(model_name='orca-mini-3b-gguf2-q4_0.gguf')\n",
        "with model.chat_session():\n",
        "    response1 = model.generate(prompt='hello', temp=0)\n",
        "    response2 = model.generate(prompt='write me a short poem', temp=0)\n",
        "    response3 = model.generate(prompt='thank you', temp=0)\n",
        "    print(model.current_chat_session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmt842QFrvrs"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from gpt4all import GPT4All\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "model = GPT4All('nous-hermes-llama2-13b.Q4_0.gguf')\n",
        "with model.chat_session('You are a love expert.Your name is paaji from punjab \\nBe terse.',\n",
        "                        '### Instruction:\\n{0}\\n### Response:\\n'):\n",
        "    response = model.generate('who are you?', temp=0)\n",
        "    print(response)\n",
        "    response = model.generate('what are your favorite type of  wommen ?', temp=0)\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljDEeD4Trvo8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkz4WQ5LrvmJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d2c9746c31e44afb685d958cc9f349a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c69a7128c7e4d1c81e0bafe759c9643": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "wizardlm-13b-v1.2.Q4_0.gguf",
              "mistral-7b-openorca.Q4_0.gguf",
              "gpt4all-falcon-newbpe-q4_0.gguf",
              "orca-mini-3b-gguf2-q4_0.gguf",
              "gpt4all-13b-snoozy-q4_0.gguf",
              "replit-code-v1_5-3b-newbpe-q4_0.gguf",
              "nous-hermes-llama2-13b.Q4_0.gguf"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Select model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_49407c60ae3a4b909b172eb409b879e8",
            "style": "IPY_MODEL_0d2c9746c31e44afb685d958cc9f349a"
          }
        },
        "49407c60ae3a4b909b172eb409b879e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
